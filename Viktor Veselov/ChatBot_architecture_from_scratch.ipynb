{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTckCM4bfM-v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Sample data preprocessing (you may want to add more depending on your specific needs)\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['background'] + ' ' + df['question'] + ' ' + df['answer'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def tokenize_and_pad(texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
        "    return padded\n",
        "\n",
        "background_padded = tokenize_and_pad(df['background'])\n",
        "question_padded = tokenize_and_pad(df['question'])\n",
        "answer_padded = tokenize_and_pad(df['answer'])\n",
        "\n",
        "# Split the data\n",
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(background_padded, question_padded, answer_padded, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "7mGz7wVsfPdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads):\n",
        "    # Normalization and Attention\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=0.4)(x, x)\n",
        "\n",
        "    # Add Skip Connection\n",
        "    x = tf.keras.layers.Add()([x, inputs])\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=4 * head_size, kernel_size=1, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=head_size, kernel_size=1)(x)\n",
        "    return tf.keras.layers.Add()([x, inputs])\n",
        "\n",
        "# Input Layers\n",
        "input_background = tf.keras.layers.Input(shape=(100,))\n",
        "input_question = tf.keras.layers.Input(shape=(100,))\n",
        "\n",
        "# Shared Embedding Layer\n",
        "shared_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=256)\n",
        "\n",
        "# Background and Question Embedding\n",
        "bg_embedding = shared_embedding(input_background)\n",
        "q_embedding = shared_embedding(input_question)\n",
        "\n",
        "# Transformer encoder\n",
        "encoded_bg = transformer_encoder(bg_embedding, head_size=256, num_heads=4)\n",
        "encoded_q = transformer_encoder(q_embedding, head_size=256, num_heads=4)\n",
        "\n",
        "# Concatenate and Final Layers\n",
        "concat = tf.keras.layers.Concatenate()([encoded_bg, encoded_q])\n",
        "output = tf.keras.layers.Dense(100, activation='softmax')(concat)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_background, input_question], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MpOWu7ChfPwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit([X1_train, X2_train], y_train, validation_data=([X1_val, X2_val], y_val), epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "id": "xtXr3iqLfPzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(background, question):\n",
        "    # Tokenize and pad background and question\n",
        "    bg_padded = tokenize_and_pad([background])\n",
        "    q_padded = tokenize_and_pad([question])\n",
        "\n",
        "    # Model prediction\n",
        "    predicted = model.predict([bg_padded, q_padded])\n",
        "\n",
        "    # Decode the predicted sequence back to text\n",
        "    answer = decode_predicted_sequence(predicted)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "8IbIfWwhfP24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KDPa9imfP4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7odHHxwfP6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9TI3bpafP88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HbqKYgFfP_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQrfZ6x7fQBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Team Project: Advanced Generative Chatbot Design\n",
    "\n",
    "TEAM 6: Bin Lu, Isaack Karanja, Victor Veselov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter notebook, which is a part of our comprehensive report on building a sophisticated chatbot capable of engaging in multi-turn conversations, adapting to context, and handling a wide array of topics.\n",
    "\n",
    "In this notebook, we aim to fine-tune an existing Language Model (LLM) from Hugging Face to enhance dialogue summarization capabilities. We will delve into the details of our review process wherein we scrutinized various models such as OPT and FlanT5 and outline the rationale behind our choice of LLaMa2-chat-instruct. This model stands out due to its high-quality instruction-tuned capabilities and inherent ability for summarizing text. \n",
    "\n",
    "To further augment the quality of inferences drawn by the model, we will be guided through a complete fine-tuning approach. Subsequently, we will assess these results using ROUGE metrics - a popular choice for evaluating automated summaries.\n",
    "\n",
    "Furthermore, we will explore Parameter Efficient Fine-Tuning (PEFT), demonstrating how it enhances the performance. Although PEFT might slightly compromise on certain performance metrics, you will observe that its benefits convincingly outweigh these minor setbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/SweatyCrayfish/Ubuntu-Lllama-2/blob/main/Final%20Folder/Model%20Selection/Photos.jpeg?raw=true\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px width:100;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [Methodology](#methodology)\n",
    "**[Dataset](#Dataset)**\n",
    "- **[Dataset Analysis](#dataset-analysis)**\n",
    "- **[Data Cleanup](#data-cleanup)**\n",
    "\n",
    "**[Model Selection](#model-selection)**\n",
    "- **[Base model vs Instructional Tuned Models](#base-model-vs-instructional-tuned-models)**\n",
    "\n",
    "**[Training](#Training)**\n",
    "- **[Full training vs PET](#Full-Fine-Tunning-vs-Parameter-Efficient-Fine-Tunning-(PEFT))**\n",
    "- **[Time, Memory Implications](#time-memory-implications)**\n",
    "\n",
    "**[Conclusion]()**\n",
    "- **[Future work]()**\n",
    "- **[References]()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from evaluate import load\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [Dataset](#Methodology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset \n",
    "We download the proprocessed dataset from hugginface and assign it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ubuntu_question_answer = load_dataset(\"mugithi/ubuntu_question_answer_jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a9e82ed3ab44dea99f3e02afadc1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498eef01b258410b966cdcdd58bcfc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, ubuntu_question_answer in ubuntu_question_answer.items():\n",
    "    ubuntu_question_answer.to_json(f\"{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdedfe5d8f840ecb4781ec01a44a197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a002becb674411aa52f568c22724cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c97c6cd268a45a9aace1d8f1e740e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433e9240881b479c93dcce7ae4265711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4101be4fae40f7a2f82d5b1951bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0db3b0828444600801e590bb010c95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files='train.jsonl',split='train')  \n",
    "test_dataset = load_dataset('json', data_files='test.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for LLaMa2 fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ Supervised Fine-Tuning (SFT) to refine LLaMa2 for the Question & Answer (Q&A) task. Within the SFT framework, we will adjust the model using a dataset of instruction and response pairs derived from the `ubuntu_question_answer` dataframe. The objective is to minimize the discrepancy between the generated answers and the actual responses, which will serve as labels. This process will also enable the model to acclimate to the response nuances present in our training data.\n",
    "\n",
    "A formatting function `formatting_func` will be utilized to process each example from the `ubuntu_question_answer` dataframe, crafting a structured prompt that will be used to fine tune LLaMa2 effectively.\n",
    "\n",
    "Furthermore, we will leverage Hugging Face's SFTTrainer function during the training phase. This function, accepting `formatting_function`, `training`, and `test` datasets allow us to focus on preparing your dataset and structuring your training examples correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(dataset):\n",
    "    text = f\"###question: {dataset['question']} , ###answer: {dataset['answer']}\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [formatting_func(example) for example in train_dataset][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###question: hey people! i have a text file of 250mb, and i need to insert some text at the beginning of the file and the end. how would i do it, not opening the whole file? ) sorry for the noobness ) , ###answer: you can do this echo test | cat - file > new file'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLaMa2 tokenizer processes the `example[0]` text  and carries out the following steps to ready it for the LLaMa2 model:\n",
    "\n",
    "- **Raw Text -> Byte-Pair Encoding (BPE) Tokenization:** Initially, the text is segmented into subwords or tokens using Byte-Pair Encoding (BPE) method, which iteratively merges the most common pair of characters or character sequences.\n",
    "\n",
    "- **Add Special Tokens:** Special tokens like`<s>` (Beginning of Sequence) and `</s>` (End of Sequence) are incorporated at the start and finish of the token sequence to indicate the boundaries of a sequence.\n",
    "\n",
    "- **Convert Tokens to IDs:** Each token is mapped to its corresponding ID based on the LLaMa2 vocabulary.\n",
    "\n",
    "- **Padding or Truncation (if required):** Unlike BERT, LLaMa2 doesn’t have a specified padding token. We set the pad_token to `<pad>` and specified its value in the model `model.config.pad_token_id`\n",
    "\n",
    "- **Create Attention Mask:** An attention mask can be created to distinguish actual tokens from any padding, ensuring the model focuses only on the real content during processing. This step may require custom handling due to the absence of a designated padding token in LLaMa2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "model_name =\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, add_eos_token=True, spaces_between_special_tokens=True)\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "tokenizer.pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════╤═════════════╤══════════════════╕\n",
      "│ Tokens     │   Token IDs │   Attention Mask │\n",
      "╞════════════╪═════════════╪══════════════════╡\n",
      "│ <s>        │           1 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁###       │         835 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ question   │       12470 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ :          │       29901 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁he        │         540 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ y          │       29891 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁people    │        2305 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ !          │       29991 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁i         │         474 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁have      │         505 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁a         │         263 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁text      │        1426 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁file      │         934 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁of        │         310 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁          │       29871 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ 2          │       29906 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ 5          │       29945 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ 0          │       29900 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ mb         │        8337 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ,          │       29892 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁and       │         322 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁i         │         474 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁need      │         817 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁to        │         304 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁insert    │        4635 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁some      │         777 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁text      │        1426 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁at        │         472 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁the       │         278 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁beginning │        6763 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁of        │         310 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁the       │         278 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁file      │         934 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁and       │         322 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁the       │         278 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁end       │        1095 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ .          │       29889 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁how       │         920 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁would     │         723 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁i         │         474 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁do        │         437 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁it        │         372 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ,          │       29892 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁not       │         451 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁opening   │        8718 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁the       │         278 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁whole     │        3353 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁file      │         934 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ?          │       29973 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁)         │        1723 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁sorry     │        7423 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁for       │         363 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁the       │         278 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁no        │         694 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ob         │         711 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ness       │        2264 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁)         │        1723 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁,         │        1919 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁###       │         835 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ answer     │       12011 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ :          │       29901 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁you       │         366 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁can       │         508 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁do        │         437 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁this      │         445 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁echo      │        2916 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁test      │        1243 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁|         │         891 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁cat       │        6635 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁-         │         448 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁file      │         934 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁>         │        1405 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁new       │         716 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ ▁file      │         934 │                1 │\n",
      "├────────────┼─────────────┼──────────────────┤\n",
      "│ </s>       │           2 │                1 │\n",
      "╘════════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "def sentence_encoding(example):\n",
    "\n",
    "  train_encodings = tokenizer(example, padding=True, return_attention_mask=True)\n",
    "  token_id = train_encodings['input_ids']\n",
    "  attention_mask = train_encodings['attention_mask']\n",
    "    \n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id))\n",
    "  token_ids = [i if isinstance(i, int) else i.item().strip() for i in token_id]\n",
    "  attention = [i if isinstance(i, int) else i.item().strip() for i in attention_mask]\n",
    "\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention], dtype=object).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "\n",
    "sentence_encoding(example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- On printnig out the 'Tokens', 'Token IDs', 'Attention Mask' of we observed that Beginning of Sequence `<s>` and End of Sequence `</s>` Tokens were added. LLaMa2 also supports other special  `[INST]` and `[/INST]` to wrap instructions, from our research. The use of this tokens have mix results in the implementaiton so we used `[###]` character that the model mapped to `token_id = 835`  to singnal the model of the beggining of a turn in the conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Question and Question downstream task, we considered several transformer models for fine tuning. Fine tuning  pre-trained language models is a common technique in natural language processing to adapt the models for downstream tasks  since training resources required to train are out of reach for most orgnzations as shown in the table below (Naveed et, al 2023)\n",
    "\n",
    "| Model | Architecture   | Parameters | Layers | Attention Heads | Processing Units | Training Unit Type    | Creator | Training Data                                       |\n",
    "|-------|----------------|------------|--------|-----------------|------------------|--------------|----------|-----------------------------------------------------|\n",
    "| T5    | Encoder-decoder| 11 billion | 24     | 128             | 1024             | TPU v3       | Google   | C4 dataset                                          |\n",
    "| OPT   | Causal-Decoder-only    | 175 billion| 96     | 96              | 992              | 40GB A100 GPU| Meta     | Pile, PushShift Reddit                              |\n",
    "| LLaMA2 | Causal-Decoder-only    | 65 billion | 80     | 64              | 2048             | 80GB A100 GPU| Meta     | CommonCrawl, C4, GitHub, Wikipedia, Books, arXiv, StackExchange |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMa2 an open-source Large Language Mode from Meta AI has been optimized for dialogue use cases (Touvron, et al, 2023) and outperforms open-source chat models on most benchmarks tested, indicating a strong performance in a dialogue environment. The OPT model also from Meta series is comparable in performance to GPT-3, which is known for its strong performance in various NLP tasks including conversational AI. The T5 model from Google, particularly when fine-tuned, achieves state-of-the-art results on many NLP benchmarks and is also capable of being fine-tuned for conversational tasks, indicating robust performance. (Naveed et al, 2023)\n",
    "\n",
    "LLaMa2 and Flan-T5 were designed with fine-tuning in mind as opposed to OPT which althrough has good performance, its creators did not provide alot of information on finetuning. \n",
    "\n",
    "\n",
    "**reference** \n",
    "\n",
    "- Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., & Mian, A. (2023). A Comprehensive Overview of Large Language Models. arXiv. https://doi.org/10.48550/arXiv.2307.06435\n",
    "\n",
    "- Touvron, H., Martin, L., Stone, K., et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv. https://doi.org/10.48550/arXiv.2307.09288\n",
    "\n",
    "- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv. https://doi.org/10.48550/arXiv.2302.13971\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrunctionally tuned\n",
    "\n",
    "Instructionally tuned models are a subtype of fine-tuned language models specially trained to follow natural language instructions to solve a task. They are fine-tuned with input-output pairs that include instructions and attempts to follow those instructions, making them adept at understanding and executing tasks as instructed in natural language. This training approach is supervised and helps improve the model's zero-shot performance on unseen tasks (Wei et al, 2021). For both the LLaMa2 and T5 models, we evalauted the instruct tuned models. \n",
    "\n",
    "| Instruct Fine Tuned Variant  | Model Type   | Number of Parameters |\n",
    "|----------------|--------------|----------------------|\n",
    "| FLAN-T5-Small  | FLAN-T5      | 80 Million           |\n",
    "| **FLAN-T5-Base**   | **FLAN-T5**      | **250 Million**          |\n",
    "| FLAN-T5-Large  | FLAN-T5      | 780 Million          |\n",
    "| FLAN-T5-XL     | FLAN-T5      | 3 Billion            |\n",
    "| **LLaMa2-Chat-7B** | **LLaMa2-Chat**  | **7 Billion            |\n",
    "| LLaMa2-Chat-13B | LLaMa2-Chat  | 13 Billion           |\n",
    "| LLaMa2-Chat-70B| LLaMa2-Chat  | 70 Billion           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- We evaluated all three models, Flan-T5, OPT and LLaMa2 and decided to focus our experiemnts on LLaMa2 becuase of its state of the art parformance, higher context length of 4096 tokens vs T5 model 512 tokens which affect its ability to generate coherent responses in conversation or other tasks requiring an understanding of broader contex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this section, we investigate the use of LLaMa2, a pre-trained Language Model, for a Question & Answer task as a Subject Matter Expert (SME) on an Ubuntu Forum. Our aim is to develop a chatbot by fine-tuning LLaMa2 to suit this specific task. \n",
    "\n",
    "We will deliberate on various alternatives that we explored to accomplish this task, including the full fine-tuning of the model. Subsequently, our decision to employ the LoRA technique will be elucidated. Finally, we will evaluate and compare the performance of our model before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-Tunning vs Parameter Efficient Fine-Tunning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning involves taking a model that has been pre-trained on a large general domain corpus and further training it on data from the target task. This allows the model to retain the general language knowledge acquired during pre-training while adapting to the nuances and specifics of the new task.\n",
    "\n",
    "In full fine tuning, all of the model's parameters are updated during the process. While this typically yields great performance, it also requires significant compute resources and introduces challenges for model scaling and storage. To address this, recent work has explored more parameter efficient fine tuning (PEFT) methods that only update a subset of the model's parameters.\n",
    "\n",
    "PEFT methods fall into a few categories as summarized below \n",
    "- **LoRA:** LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune pre-trained language models by introducing a low-rank adaptation layer. This adaptation layer is added to the pre-trained model and only the parameters of this layer are fine-tuned during the adaptation process.\n",
    "- **Prefix Tuning:** This technique is used to fine-tune language models by only updating a fixed-length prefix of the input sequence. The prefix can be considered as a form of continuous prompt that is optimized during the training process.\n",
    "- **P-tuning:** P-tuning, or Parameterized-tuning, is a fine-tuning approach where the prompts are parameterized and optimized during the training process. Unlike fixed textual prompts, these parameterized prompts can adapt to the task at hand.\n",
    "- **Prompt Tuning:** This is a method where you fine-tune the model by optimizing over a set of fixed textual prompts. Unlike P-tuning, the prompts here are not parameterized.\n",
    "- **Full Fine-Tuning:** In full fine-tuning, the entire model, including all its parameters, is fine-tuned for a specific task.\n",
    "\n",
    "| Method          |  Explanation                                                                                   | Pros                                                     | Cons                                                   | Comparison with Full Fine-Tuning            |\n",
    "|-----------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------|--------------------------------------------------------|--------------------------------------------|\n",
    "| [LoRA](https://arxiv.org/abs/2106.09685)            | Adds a low-rank adaptation layer for fine-tuning. Only this layer's parameters are updated.             | 1. Computationally efficient<br>2. Effective fine-tuning  | 1. May not capture complex task-specific requirements  | Less parameter-intensive than full fine-tuning |\n",
    "| [Prefix Tuning](https://arxiv.org/abs/2101.00190)   | Optimizes a fixed-length prefix of the input sequence during fine-tuning.                                | 1. Efficient for sequence generation tasks<br>2. Can adapt to various tasks  | 1. Limited to sequence tasks                          | More task-specific but less general than full fine-tuning |\n",
    "| [P-tuning](https://arxiv.org/abs/2103.10385)        | Parameterizes and optimizes the prompts during fine-tuning.                                              | 1. Highly adaptable to tasks<br>2. Can improve prompt efficacy  | 1. May require careful initialization                 | More adaptable but may require more tuning than full fine-tuning |\n",
    "| [Prompt Tuning](https://arxiv.org/abs/2104.08691)   | Fine-tunes over a set of fixed textual prompts.                                                          | 1. Easy to implement<br>2. Useful for specific tasks     | 1. Not adaptable<br>2. Limited to known prompts         | Less adaptable and more constrained than full fine-tuning  |\n",
    "| Full Fine-Tuning| Fine-tunes all the parameters of the model for a specific task.                                          | 1. Highly effective for task-specific requirements<br>2. Can capture complex features  | 1. Computationally expensive<br>2. Risk of overfitting  | Most comprehensive but resource-intensive  |\n",
    "\n",
    "In this section of the report, we will delve into the application of LoRa in our Jupyter notebook with the aim of attaining substantial reductions in several areas. This includes the volume of data required, the time consumed during training, the computational requirements necessary for processing, and the overall cost associated with these operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "LoRA stands for Low-Rank Adaptation, a technique to efficiently fine-tune large pre-trained language models. Below is how it works\n",
    "\n",
    "**Components of LoRa:**\n",
    "- **Adapter Matrices:** LoRa introduces low-rank adapter matrices that interface with specific weight matrices in the pre-trained model. In the case of Transformer models, these adapter matrices are added in parallel to the Query, Key, and Value matrices in each attention layer.  \n",
    "- **Low-Rank Structure:** \n",
    "The adapter matrices are \"low-rank,\" meaning they can be represented with far fewer parameters than the original weight matrices. For example, if the original matrix has a rank of 4096, the adapter might have a rank of just 1 or 2.\n",
    "\n",
    "**Training Process:**  \n",
    "- **Initialization:** The adapter matrices are randomly initialized while the original weight matrices remain fixed.\n",
    "- **Fine-Tuning:** During the fine-tuning process, only these adapter matrices are updated. They capture essential new task-specific knowledge. \n",
    "- **Task Specialization:** The original weight matrix provides broad, generalized knowledge. In contrast, the low-rank adapter focuses on specialized features needed for the new task.\n",
    "\n",
    "**Deployment:**  \n",
    "- **Inference:** When the model is deployed for making predictions, the adapter matrices can be combined with the original weight matrices without affecting the inference time. \n",
    "- **Adaptability** the adapters can also be easily swapped for different tasks\n",
    "\n",
    "**Benefits:**  \n",
    "- **Efficiency:** Requires much less computational power and memory than traditional fine-tuning methods.\n",
    "- **Less Overfitting:** By focusing on a small set of parameters, LoRa reduces the risk of overfitting, especially when fine-tuning data is limited.\n",
    "- **Strong Performance:** Experiments have shown that LoRa can achieve similar or better performance compared to traditional fine-tuning methods, despite using significantly fewer trainable parameters.\n",
    "- **Data Efficient:** LoRa has shown strong performance in adapting large models like GPT-3, RoBERTa, and DeBERTa using only thousands to tens of thousands of examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Lora Memory Usage\n",
    "\n",
    "### Memory\n",
    "\n",
    "To train our model, we needed to factor in the model weights, activations, optimizer states, and additional memory for elements like mini-batch data, regularization, and algorithm overhead summarized in the table below for a typical state of the art langauge models such as LLaMa2 stored in full precision. For a 7B parameter.\n",
    "\n",
    "\n",
    "| Item (Full Precision)                         | Memory Usage (bytes per parameter) |\n",
    "|------------------------------|------------------------------------|\n",
    "| Model Weights             | 4 (32bit)                                  |\n",
    "| AdamW Optimizer (2 states)   | +8                                 |\n",
    "| Gradients                    | +4                                 |\n",
    "| Activations and Buffer       | +8 (based on parameter sequence length, hidden size, and batch size) |\n",
    "\n",
    "For the LLaMa-7B  model, for full training of the LLaMa 7B model, the memory requirement comes out to 160 GB which requires at last 2 NVidia 80G A100s\n",
    "\n",
    "\n",
    "| Item (Optimized)                                 | Memory Usage (bytes per parameter) |\n",
    "|--------------------------------------------------|------------------------------------|\n",
    "| Model Weights (half precision bf16)           | .5 (4bit with nf4 quantization)    |\n",
    "| AdamW Optimizer (2 states) with bitsandbytes     | +4                                 |\n",
    "| Gradients                                        | +2                                 |\n",
    "| Activations and Buffer                           | +1 (Optimized: gradient_checkpointing ) |\n",
    "\n",
    "By adding multipe optimizations the training memory requirement comes down to resonable amount that can fit into a single GPU \n",
    "\n",
    "**QLoRA**\n",
    "\n",
    "- We made use Quantiaized LoRA which include the following innovations over LoRA __4-bit quantization__, the introduction of __4-bit NormalFloat (NF4)__, __double quantization__, __paged optimizers__, and __backpropagation through quantized weights__ to fine-tune the model, which collectively reduce memory usage and facilitate fine-tuning large models on limited hardware.\n",
    "\n",
    "- Imagine a tall baseball player who suddenly becomes short (quantization). This change demands adjustments like getting smaller shoes (4-bit NormalFloat) to fit better. Further, the player needs to alter his training regime (backpropagation through quantized weights to adapters added at every layer) to adapt to his new height, ensuring his performance remains top-notch despite the physical change. Similarly, QLoRA makes necessary adjustments to maintain performance while reducing the model's \"size\" (memory requirement).\n",
    "As we will see, this results in a smaller model that can is memorry\n",
    "\n",
    "**reference**\n",
    "- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. https://doi.org/10.48550/arXiv.2106.09685\n",
    "\n",
    "- Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. https://doi.org/10.48550/arXiv.2305.14314\n",
    "<img src=\"https://github.com/SweatyCrayfish/Ubuntu-Lllama-2/blob/main/Final%20Folder/Model%20Selection/QLora%20_1.jpeg?raw=true\"\n",
    " alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 20px width:100;\" />\n",
    "     \n",
    "     \n",
    "\n",
    "For serving, you typically only need to consider the model weights and for the 7B model you only require 10GB with 7GB for a 8Bit quantized model and LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"Nvidia SMI reported GPU memory occupied: {info.used//1024**3} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_giga_bytes(model):\n",
    "    mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "    mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs # in bytes\n",
    "    gpu_Gbytes =  mem / 1024 / 1024 / 1024\n",
    "    print(f\"Mem Prams + Mem Buffer used Calculated Model Memory: {round(gpu_Gbytes,2)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nvidia SMI reported GPU memory occupied: 0 GB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- Before loading the model to memory, the  Nvidia SMI reported GPU memory occupied consumed is ~zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the base model from hugginface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10927c9c3ff4a6388d210eb4ea5252c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name =\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "original_model = LlamaForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "original_model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, spaces_between_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem Prams + Mem Buffer used Calculated Model Memory: 3.57 GB\n",
      "Nvidia SMI reported GPU memory occupied: 5 GB.\n"
     ]
    }
   ],
   "source": [
    "model_giga_bytes(original_model)\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- On loading the  7Billion 4bit quantiazed model, the   calculated model memory stood at 3.57 GB, inclusive of Model Parameters and  Model Buffer, the Nvidia SMI reported a significantly higher GPU memory occupation of 5 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "`r=16` Rank relates to how many dimensions worth of new knowledge the adapter can represent.Low rank is like zooming out - you only see the big, rough picture. High rank is like zooming in - you see more fine-grained details with Rank number like adapter matrix resolution. Higher Rank requires more parameters. Rank `1-4` often works well in practice.  \n",
    "\n",
    "`lora_alpha=64` = Scales the entire adapter matrix values during training which affect how rapidly the adapters can adapt the model, acting effectively like learning rate. Higher learning rate reduces the stability. \n",
    "\n",
    "`bias=\"lora_only\"`In LoRA, the low-rank adapter matrices are inserted in parallel to the original weight matrices of the model. By default, these adapter matrices include trainable bias terms. Setting `none` removes the bias terms from the adapter matrices. The other options are `lora_only` option can be useful if you want to disable bias in the original model (e.g. for regularization) but still allow the adapters to learn bias and `all` (default) enables bias in both the fixed original model and the adaptable LoRA modules.\n",
    "\n",
    "                                            `output = input @ weight_matrix + bias`\n",
    "\n",
    "**Differnces wbetween LoRA and QLoRA**\n",
    "\n",
    "In the paper **Lora: low-rank adaptation of large language models** (Hu et al, 2023) suggest a bias value of `none` for most of their examples and use LoRA adapters that targets to only the key, value and query adapters. In QLoRA (Dettmers et al, 2023), the authors target the  all the key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers with LoRA adapters. They also update all bias in the model vs keeping it frozen. This can be intutively understood as a baseball who suddenly finds himself short, they would player who needs adjustments many more areas in order to perform at previous level.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create `Trainer` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the LoRA adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import LoraConfig, get_peft_model,  TaskType\n",
    "from typing import List\n",
    "\n",
    "max_seq_length = 512\n",
    "micro_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "lora_r: int = 16\n",
    "lora_alpha: int = 64\n",
    "lora_dropout: float = 0.1\n",
    "cutoff_len: int = 256\n",
    "lora_target_modules: List[str] = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\"\n",
    "  ],\n",
    "use_wandb = True\n",
    "wandb_run_name=\"LLaMav2_Train_01\"\n",
    "max_steps = 1000\n",
    "\n",
    "\n",
    "# Define LoRA Config\n",
    "LoraConfig = LoraConfig(\n",
    " r=lora_r,\n",
    " lora_alpha=lora_alpha,\n",
    " target_modules=lora_target_modules[0],\n",
    " lora_dropout=lora_dropout,\n",
    " bias=\"all\",\n",
    " task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,392,576 || trainable%: 0.589770503135875\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, LoraConfig)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- With QLoRA, we train only 29.5% of the model's parameters, which equates to 19,988,480 trainable parameters out of the total 6,778,392,576 parameters in the full model. This decreases our memory footprint by over 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad91ba0fede5432ca2c229ff71c27565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7eaacaf8b7474fba5c2cc4ea841a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "output_dir = f'.log/peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    # auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=2e-4, # Higher learning rate than full fine-tuning.\n",
    "    max_steps=max_steps,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,                # Save checkpoints every 50 steps\n",
    "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "    report_to=\"wandb\",\n",
    "    eval_steps=20, \n",
    "    run_name=wandb_run_name if use_wandb else None,    \n",
    ")\n",
    "    \n",
    "peft_trainer = SFTTrainer(\n",
    "    model=original_model,\n",
    "    peft_config=LoraConfig,\n",
    "    max_seq_length=max_seq_length,\n",
    "    formatting_func=formatting_func,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmugithi\u001b[0m (\u001b[33mnyumbani\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/nlp/Module 7/wandb/run-20231024_013625-pp75yws7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nyumbani/huggingface/runs/pp75yws7' target=\"_blank\">LLaMav2_Train_01</a></strong> to <a href='https://wandb.ai/nyumbani/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nyumbani/huggingface' target=\"_blank\">https://wandb.ai/nyumbani/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nyumbani/huggingface/runs/pp75yws7' target=\"_blank\">https://wandb.ai/nyumbani/huggingface/runs/pp75yws7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 50:09, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>4.741363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>5.961457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>6.324196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>6.434465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>6.455405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>6.486933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>6.529018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>6.551917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>6.600418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.607201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>6.635504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.665953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.688008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>6.687768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>6.729610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.732814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.764187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.766061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.797087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.789788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.814527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.837826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.834909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.861438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.873124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.875279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.885690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.888018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.903415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.914389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.920438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>6.926609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.942974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.942226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.955058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.955811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.969064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.978432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.982751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.986789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.993363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>6.996312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>7.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>7.005310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>7.014045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>7.017101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>7.016684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>7.018682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>7.017358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>7.017203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.037262808330357076, metrics={'train_runtime': 3026.3655, 'train_samples_per_second': 5.287, 'train_steps_per_second': 0.33, 'total_flos': 2.65467394523136e+17, 'train_loss': 0.037262808330357076, 'epoch': 1000.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- The training report indicates that a LoRA adapter was prepared for inference post-training, with a 30-minute training duration. \n",
    "- The training loss diminished significantly from 2.05 to 0.0065 across 100 steps, while the validation loss increased from 3.14 to 6.42. \n",
    "- The final output showcases a training loss of 0.623 with metrics such as a training runtime of 313.2791 seconds and a total floating point operations (FLOPs) of approximately 2.65e+16. - - This suggests an initial learning phase with subsequent overfitting, as evidenced by the diverging validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging with base model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we integrate LoRA adapters with the base model to utilize them effectively. Initially, the adapters will be saved to disk, followed by the deletion of the base model to free GPU capacity. Subsequently, the adapter will be reloaded and combined with the base model prior to employing it for inference tasks. This procedure ensures efficient utilization of resources while preparing the model for inference operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the LoRA adapter to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_adapter_dir/tokenizer_config.json',\n",
       " 'lora_adapter_dir/special_tokens_map.json',\n",
       " 'lora_adapter_dir/tokenizer.model',\n",
       " 'lora_adapter_dir/added_tokens.json',\n",
       " 'lora_adapter_dir/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_adapter_dir = \"lora_adapter_dir\"\n",
    "peft_trainer.save_model(lora_adapter_dir)\n",
    "tokenizer.save_pretrained(lora_adapter_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free memory for merging weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del original_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the adapter and the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRa configuration from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_adapter_config = LoraConfig.from_pretrained(lora_adapter_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d469ae845ace48ff8ea020702ab7973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(lora_adapter_config.base_model_name_or_path, device_map=\"auto\", quantization_config=bnb_config)\n",
    "base_model.config.use_cache = False\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(lora_adapter_config.base_model_name_or_path, add_eos_token=True, spaces_between_special_tokens=True)\n",
    "trained_tokenizer.padding_side = \"left\"\n",
    "trained_tokenizer.pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the lora adapter and base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_and_lora_adapter = PeftModel.from_pretrained(base_model, lora_adapter_dir,  device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "print(lora_adapter_config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fe5b80846c4e5c83e91055d862a1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orignal_model = AutoModelForCausalLM.from_pretrained(lora_adapter_config.base_model_name_or_path, device_map=\"auto\", quantization_config=bnb_config)\n",
    "orignal_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the merged model and tokenizer to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As of the time of these report QLoRA  does not support merging the adapter with the base model since they are at different precisions. The idea behind QLora is to have mini-adapters that are interchagable at inference time, and in a way way to createan army-mixture of experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform human evaluation against results on from the base model and final model and compare the results for this, we will load the original_model and peform infence against the orginal model and q_lora model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample questions for human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions=[\n",
    "    \"Does ubuntu come with a firewall by default?\",\n",
    "    \"Can grub-install work with ext3?\",\n",
    "    \"Is there a CLI command to roll back updates?\",\n",
    "    \"How do I get rid of Google Chrome?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform zero shot inference on sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_zero_shot(question, model_eval, tokenizer_eval):\n",
    "    QUESTION = \"\"\"### question:\"\"\"\n",
    "    ANSWER= \"\"\"### answer:\"\"\"\n",
    "        \n",
    "    prompt = f\"{QUESTION} {question.strip()} {ANSWER}\"\n",
    "    \n",
    "    test_inputs = tokenizer_eval(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    ## Assign Inputs to GPU\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model_eval.generate(**test_inputs,\n",
    "                            max_new_tokens=64,\n",
    "                            temperature = 0.1\n",
    "                                          )\n",
    "\n",
    "    return tokenizer.decode(test_outputs[0][test_inputs['input_ids'].shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results(sample_questions, base_model, base_tokenizer, qlora_model, trained_tokenizer):\n",
    "    questions_list = []\n",
    "    base_model_results_list = []\n",
    "    qlora_model_results_list = []\n",
    "    \n",
    "    for question in sample_questions:\n",
    "        base_model_results = test_model_with_zero_shot(question, base_model, base_tokenizer)\n",
    "        qlora_model_results = test_model_with_zero_shot(question, qlora_model, trained_tokenizer)\n",
    "        \n",
    "        questions_list.append(question)\n",
    "        base_model_results_list.append(base_model_results)\n",
    "        qlora_model_results_list.append(qlora_model_results)\n",
    "    \n",
    "    data = {\n",
    "        'Question': questions_list,\n",
    "        'Base_Model_Results': base_model_results_list,\n",
    "        'QLoRA_Model_Results': qlora_model_results_list\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# \n",
    "results_df = collect_results(sample_questions, orignal_model, tokenizer, model_and_lora_adapter,trained_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  </th><th>Question                                    </th><th>Base_Model_Results                                                                                                                                                                                                                                                                                          </th><th>QLoRA_Model_Results                                                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\"> 0</td><td>Does ubuntu come with a firewall by default?</td><td>. Yes, Ubuntu comes with a firewall by default. The firewall is called &quot;Ubuntu Firewall&quot; or &quot;ufw&quot; for short. It is enabled by default on most Ubuntu installations. ufw is a simple, easy-to-use firewall that provides a powerful and flexible way to control network traffic on                           </td><td>t is enabled by default in Ubuntu. The firewall is called iptables, and it controls incoming and outgoing network traffic. To enable or disable iptables, you can use the following commands:\n",
       "\n",
       "* To enable iptables, type:\n",
       "\n",
       "sudo iptables -A INPUT -j                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\"> 1</td><td>Can grub-install work with ext3?            </td><td>t is possible to use grub-install with ext3 file system, but it requires some additional steps. Here&#x27;s how you can do it:\n",
       "\n",
       "1. First, make sure that you have the ext3 driver installed on your system. You can do this by running the following command:\n",
       "```                                                                                                                                                                                                                                                                                                             </td><td>t is possible to use ext3 as the install media for ubuntu. all you need to do is use the command:\n",
       "\n",
       "sudo grub-install /dev/disk/by-uuid /dev/disk/by-uuid ntfs-label = &quot;my ubuntu install&quot;\n",
       "\n",
       "where /dev/                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\"> 2</td><td>Is there a CLI command to roll back updates?</td><td>t There are several ways to roll back updates in Linux, depending on the update mechanism used. Here are some common methods:\n",
       "\n",
       "1. Using `apt-rollback`:\n",
       "\n",
       "`apt-rollback` is a command-line tool that allows you to roll back updates in Ubuntu and other Debian-                                                                                                                                                                                                                                                                                                             </td><td>t is possible to roll back updates in Ubuntu using the `restore&#x27; command. This command will restore the package levels to what they were before the most recent update.\n",
       "\n",
       "For example, if you updated a package called &#x27;mysql-server&#x27; and want to return to a previous version of that package, you would                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\"> 3</td><td>How do I get rid of Google Chrome?          </td><td>t is possible to remove Google Chrome from your computer, but it&#x27;s important to note that Chrome is a widely used and popular browser, and many people find it to be a convenient and reliable tool for browsing the internet. If you&#x27;re looking to remove Chrome for any reason, here are the steps you can</td><td>s the world&#x27;s most popular search engine, Google Chrome is the most widely used web browser, and for good reason. It&#x27;s easy to use, fast, and has a wide range of features that make browsing the web a breeze. However, if you&#x27;re looking to switch to a different</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "html_output = tabulate(results_df, headers='keys', tablefmt='html')\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "- Even with a limited run, you can onbserve in this notebook that the model is starting to adopt the tone of the tone of the questions in the ubuntu chat forum. For example #3. Where the user asks about uninstalling GOogle Chrome, the chatbot reponse that the question is ambigous with `sorry, i don't understand what you're trying to do` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report has presented the results of evaluating three different models - Flan-T5, OPT, and LLaMa2. The decision to focus on LLaMa2 was based on its state-of-the-art performance and its ability to handle a higher context length of 4096 tokens compared to the T5 model's 512 tokens. This aspect significantly influences a model's capacity to generate coherent responses in tasks that require understanding broader context.\n",
    "\n",
    "The memory consumption at different stages, from loading the model into memory to the post-training phase, was closely monitored. It is noteworthy that the use of QLoRa allowed for training only 29.5% of the model's parameters, which considerably reduced our memory footprint by over 70%. \n",
    "\n",
    "However, while training loss showed a significant decrease from 2.05 to 0.0065 across 100 steps, validation loss increased from 3.14 to 6.42. This suggests an initial learning phase followed by subsequent overfitting shown by diverging validation loss.\n",
    "\n",
    "QLoRa's utility lies in its interchangeability at inference time, creating a flexible system akin to an army-mixture of experts. Even with limited runs, it was observed that the model began adopting specific tones from the Ubuntu chat forum questions.\n",
    "\n",
    "In conclusion, despite some identified issues such as overfitting during training, LLaMa2 with QLoRa shows promise in handling broader contexts and reducing memory footprint during training. However, further testing and adjustments are necessary for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HuggingFace (n.d). Supervised Fine-tuning Trainer. https://huggingface.co/docs/trl/main/en/sft_trainer\n",
    "- Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLORA: Efficient Finetuning of Quantized LLMs. Arxiv. https://doi.org/10.48550/arXiv.2305.14314\n",
    "- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. https://doi.org/10.48550/arXiv.2106.09685\n",
    "- Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\n",
    "and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning\n",
    "Representations, 2021.\n",
    "- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. https://doi.org/10.48550/arXiv.2106.09685\n",
    "- Li, X., & Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Generation. Retrieved from https://doi.org/10.48550/arXiv.2101.00190\n",
    "- Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. https://doi.org/10.48550/arXiv.2104.08691\n",
    "- Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., & Tang, J. (2021). GPT understands, too. arXiv preprint arXiv:2103.10385. https://doi.org/10.48550/arXiv.2103.10385\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

## References

- Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). [QLORA: Efficient Finetuning of Quantized LLMs](https://doi.org/10.48550/arXiv.2305.14314). Arxiv.
- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). [LoRA: Low-Rank Adaptation of Large Language Models](https://doi.org/10.48550/arXiv.2106.09685).
- HuggingFace (n.d). [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer).
- Lester, B., Al-Rfou, R., & Constant, N. (2021). [The power of scale for parameter-efficient prompt tuning](https://doi.org/10.48550/arXiv.2104.08691). arXiv preprint arXiv:2104.08691.
- Li, X., & Liang, P. (2021). [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://doi.org/10.48550/arXiv.2101.00190).
- Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., & Tang, J. (2021). [GPT understands, too](https://doi.org/10.48550/arXiv.2103.10385). arXiv preprint arXiv:2103.10385.
- Lowe, R., Pow, N., Serban, I., & Pineau, J. (2016). [The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems](https://doi.org/10.48550/arXiv.1506.08909).
- Martin, C. H., Peng, T. (Serena), & Mahoney, M. W. (2021, July 5). [Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data](https://www.nature.com/articles/s41467-021-24025-8). Nature News.
- Nagyfi, R. (2023). [Open-Assistant/ubuntu_parser.ipynb](https://github.com/sedthh/Open-Assistant/blob/b3a8c2479b12ea69d66487e2852b836083b7e4db/data/datasets/ubuntu_dialogue_qa/ubuntu_parser.ipynb).
- Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://doi.org/10.48550/arXiv.2307.09288). arXiv.
- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). [LLaMA: Open and Efficient Foundation Language Models](https://doi.org/10.48550/arXiv.2302.13971). arXiv.
- Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., & Mian, A. (2023). [A Comprehensive Overview of Large Language Models](https://doi.org/10.48550/arXiv.2307.06435). arXiv.
- Ograbek, K. (2023, September 7). [How to create Llama 2 chatbot with Gradio and Hugging Face in Free Colab](https://www.youtube.com/watch?v=lSBX-nMQ8cE), [Notebook](https://colab.research.google.com/drive/1SSv6lzX3Byu50PooYogmiwHqf5PQN68E). YouTube.
- PyPI. (n.d.). [Gradio](https://pypi.org/project/gradio/).
- Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V. (2021). [Fine Tuned language models are zero-shot learners](https://doi.org/10.48550/arXiv.2109.01652).
